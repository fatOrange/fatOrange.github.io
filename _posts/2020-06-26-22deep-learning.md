---
layout: article
title: Deep learning 
tags: [deeplearning, review, 翻译,经典论文]
author: fatOrange
aside:
  toc: true
sidebar:
  nav: deeplearning
mathjax: true
mathjax_autoNumber: true
---

LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." Nature 521.7553 (2015): 436-444.⭐⭐⭐⭐⭐

### 摘要

​		深度学习允许计算模型学习多层抽象的数据表示，计算模型是由多层处理层组成的。这些模型应用于语言识别，视觉识别，物体检测，药物发现，基因组学等学科的最前沿，极大的提高了学科的发展。深度学习通过反向传播算法发现大量数据集中的复杂结构，以指示机器应该如果改变其内部参数，这些参数要根据上层参数的表示，来介绍这一层参数的表示。深度卷积网络在图片，视频，语音，音频方面有突破性的进展；递归网络在文本和语音方面大放异彩。

### 介绍

​		机器学习为现代社会的许多方面提供了动力：web 搜索，社交网络的内容过滤，电子商务网站的推荐系统，还有很多热门商品——摄像头，智能手机。用机器学习来识别图像的物体，把语音翻译成文字，根据用户的喜好来推送新闻，帖子，产品，搜索结果。这些应用越来越多的使用叫做深度学习的技术。

​		传统的机器学习技术受处理原始数据格式的能力限制。几十年来，构建一个模式识别或者机器学习系统需要一个认真仔细的工程学和大量相关领域的专家来设计特征提取器，这种特征提取器通过可以学习的子系统，通常是一个分类器，从输入中检测和分类，再将输入的原始数据(例如图片像素值)转化成合适的内部表示或特征向量。

​		表征学习 ( Representation Learning ) 允许输入原始数据到机器中，机器会自动计算"检测"和"分类"所需要的表征。深度学习方法是一种多层表征学习的方法，通过多层简单但是非线性的模块组成，每个模块将这一层(从原始输入开始)的表征转换为更抽象的高层表征。通过组成足够多这样的变换，就可以学习到非常复杂的函数。对于分类任务，高层表征放大了对区分影响大的输入，抑制了无关的输入。例如，一张以像素值为数组来表示的图片，第一层表征通常表示图片的某个方向和位置是否到了边缘。第二层通常通过标记边的特定的组合来检测图案，而不管边缘位置的细小变化。第三层会这些图案的组合一个更大的图案组合，这些图案的组合对应于这些物体的一部分，后续的层会检测这些层组合的对象。深度学习的关键在于这些特征层不是由人类工程师设计的：他们是使用通用学习过程的数据中数据得来。

​		深度学习在困扰人工智能社区多年的最佳尝试问题上取得了重大的进展。结果证明深度学习在发现高纬数据的内部复杂结构有优势。因此适用于科学，商业和政务的很多领域。除了打破了图像识别和语音识别的记录，他还在预测潜在药物分子活性，分析粒子加速数据，重构脑回路和预测在基因表达和疾病中非编码DNA的突变的影响中击败了其他机器学习技术。也行更令人惊讶的是，在自然语言处理，理解，特别是主题分类，情感分析，问题回答，语言翻译中，深度学习都产生了很有前景的结果。

​		我们认为深度学习在最近几年还会取得更多的成功，因为他只要求很少的手动的去构建，所以他能轻易发挥可用计算和数据增加的优势。目前正在为深度学习开发的新的学习算法和结构只会继续扩大这种优势。

### 监督学习

​		监督学习( supervised learning ) 是机械学习和深度学习最常见的形式。想象一下，我们想要构建一个系统可以对图像分类，包括房子，汽车，人或者宠物。我们第一步需要收集大量的房子，汽车，人和宠物的数据集，每一个类都有一个标志。在训练过程中，机器都会先被展示一张图片，然后以向量的形式对于每一个分类打分并输出。我们想要期望分类在所有分类中得分最高，但是这不可能在训练之前发生。我们计算目标函数( objective function )来衡量输出分数和期望分数之间的误差( 或者距离 )。然后机器会修改内部可调节的参数来减少误差。这些被称为"权重( weight )"的可调节的误差是一个实数，可以看作是定义机器输入输出函数的 "旋钮( knobs )"。在经典的深度学习系统中，也许有数以亿计可调节的权重和数以亿计训练机器的打过标签的例子。

​		为了适当的调整权重，学习算法会对每一个权重计算一个梯度向量，梯度向量表示如果权重增加了非常小的量，误差应该如何变化，是增加多少还是减少多少。然后权重向量会安装梯度向量想法的方向进行调整。

​	    目标函数对所有训练样本进行平均，他可以被视作高纬权重值空间中的一种丘陵景观。负梯度向量表示这种景观中最陡峭的下降方向，使其能接近最小值，平均意义上输出的误差较低。

​		在实践中，大多数从业者使用一种称为随机梯度下降（SGD）的程序。这包含显示几个用例的输入向量，计算输出和误差，计算这些用例的平均梯度，并调整相应的权重。这个过程会一直在训练集一些小的用例上重复，直到目标函数的平均值不在下降。之所以被叫做随机是因为每一个小的用例集都会给所有用例的平均梯度一个随机的估计。与复杂的多的优化算法相比，这种简单的过程都能找到一组较好的权重。训练之后不同用例组成的用例集称为测试集，测试集被用来衡量系统的性能。这是为了测试机器的泛化能力，这种泛化能力可以在输入训练时从未见过的输入时产生一个较为合理的答案。

​		许多当前机器学习的应用都是使用手工特征的基础上使用线性分类器。两类的线性分类器计算特征向量分量的加权和。如果加权和大于阈值，输入就会被分类为摸一个特定类别。

​		自从20世纪60年代起，我们就知道线性分类器只能将输入空间划分为非常简单的区域，被称为超平面分隔的半空间。但是诸如图片和语音识别，要求输入输出函数对输入的无关变量不敏感。比如位置的信息，方向，物体的光照，语音中语调和口音的。但是要求输入输出函数对特定细小的变量敏感(比如，白狼和一种叫做萨摩耶的像狼的白狗之间的差距)在图像层面，两张不同姿势和不同环境的萨摩耶的图片是有差距的，但是，两张统一姿势和背景的萨摩耶和狼狗是很相似的，一个线性分类器，或任何其他“浅”分类器操作的原始像素不可能区分后两个，而把前两个放在同一类别。这就是为什么浅层分类器需要一个好的特征提取器来解决选择性-不变性困境-一个能够产生对图像的各个方面有选择性的表示。它能解决选择性-不变性的难题--它能产生对图像中对识别很重要的方面有选择性的表征，但对不相关的方面（如动物的姿势）是不变的。

​		深度学习框架是简单模块组成的多层堆叠，所有(或大部分)模块都受学习影响。其中很多模块计算非线性的输入输出映射。每个栈中模块会对它的输入进行转换，以此来增加选择性和不变性的表征。一个有多层非线性层(比如说5层到20层)的系统可以实现极度复杂的功能，他的输入同时能对各种细小的细节敏感——区分萨摩耶和白狼——并对大量无关的变量不敏感，比如背景，姿势，灯光和环境对象。

![Nhm1HI.png](https://s1.ax1x.com/2020/06/29/Nhm1HI.png)

a. 一个可以扭曲输入空间造成线性分类(例如红线和蓝线)的多层神经网络(连接点)。注意，输入空间中规则的格子(左边)被隐藏层(右边)转化了。这个例子中只有两个输入层，两个隐藏层，一个输出层，但是用于物体检测或者自然语言处理包含十万或者百万个单元，经[C. Olah (http://colah.github.io/)](http://colah.github.io/)授权转载。

![NhnrsH.png](https://s1.ax1x.com/2020/06/29/NhnrsH.png)

b.导数的链式法则告诉我们，导数的导数(x在y上的变化，y在z上的变化)是如何计算的。Δx对Δy产生影响，首先要Δx通过乘以∂y/∂x(即偏导数的定义)。同理，Δy也对Δz有影响。将一个Δz中的Δy进行代换，就得到了链式法则，Δx通过乘以∂y/∂x再乘以∂z/∂y得到Δz。当x，y和z是向量时(导数是Jacobian矩阵)，链式法则也同样有效。

![NhnsLd.png](https://s1.ax1x.com/2020/06/29/NhnsLd.png)

c.在一个两层隐藏层，一个输出层，每一层都可以正向或者反向传播梯度，从而组成一个模块。每一层，我都首先计算每个单元的总输入z，它是下面一层单元输出的加权和。然后将一个非线性函数f(.)应用于z，得到单元的输出。为了简化，我们省略了偏置项。神经网络中的非线性函数包括，近些年经常被使用的修正线性单元(RuLU)f(z)=max(0,z),以及比较传统的sigmoid函数，比如双曲正切函数，f(z)=(exp(z)-exp(-z))/(exp(z)+exp(-z))，和logistic函数,f(z) = 1/(1+exp(-z))。

![NhnDQe.png](https://s1.ax1x.com/2020/06/29/NhnDQe.png)

d.用于计算反向传播的等式。在每个隐藏层中，我需要计算对应输出单元的导数误差，每一层个单元的误差又是上一层这个单元总输入的加权平均再求导。然后，我们将相对于输出的误差导数乘以f(z)的梯度，转换成相对于输入的误差导数。在输出层，对应单元输出的误差偏导是通过对损失函数求微分计算得来。如果$$l$$的损失函数是 $$0.5{({y_l} - {t_l})^2}$$ 对它求导就是$${y_l} - {t_l}$$ , $${t_l}$$是目标值。一旦$$\partial E/\partial {z_k}$$已知,下一层连接单元 $$j$$ 的权重 $${w_{jk}}$$ 误差的导数是 $${y_j}\partial E/\partial {z_k}$$ 。



![NIATEt.png](https://s1.ax1x.com/2020/06/30/NIATEt.png)

**图2|卷积网络的内部** . 一个典型的卷积网络结构的每一层(水平方向)的输出(不包括滤波器)应用于萨摩耶狗的图像(左下角；RGB输入，右下)每个矩阵图像，都是一个特征图，对应于每个图像位置检测到的一个学习特征的输出。信息流自下而上，低层特征作为定向边缘检测器，并为输出的每个图像类计算分数。ReLU，整流线性单元。

### 训练多层架构的反向传播

​		从模式识别的早期阶段，研究人员就想要用可以训练的多层网络代替手工提取特征的方法。但是尽管十分简单，这种方法直到20世纪80年代才被广泛理解。但事实证明, 用随机下降梯度法训练多层网络就行了。只要模块的输入和他内部权重相对平滑，就可以用反向传播计算梯度。这种想法是可行的也是有效的。是由几个不同的小组在20世纪70年代和80年代独立发现的。

​		反向传播算法用于计算相对模块多层栈的权重的目标函数的梯度，就是一个计算链式法则的实际应用。关键是，相对于某个模块输入的目标函数的导数(或梯度)，可以通过对应模块输出(或者后续模块的输入)的导数的反向求导来获得(Fig. 1)。在输出层，相对于单位输出的误差导数是通过微分成本函数计算出来的。

​		深度学习的许多应用都使用的前馈神经网络架构(图.1),它学习将一个固定大小的输入(例如，图片)映射到固定大小的输出(例如，不同类的概率)上面，为了将参数从上一层到下一层，一组计算单元会计算从上一层传到自己输入层的加权和，并将结果传递到非线性函数中。目前，最流行的非线性函数是线性整流单元(ReLU)，简单来说，就是半波整流 $$f(z)=max(z,0)$$ ,在过去的几十年中，神经网络使用了更加平滑的非线性函数，比如 $$tanh(z)$$ 或者  $$\frac{1}{{1{\rm{ + }}{e^{ - z}}}}$$ 但ReLU通常在具有多层的网络中学校的更快，允许在没有无监督预训练的情况下训练深度监督网络。不在输入层或者输出层的单元习惯上被称为隐藏单元。隐藏层可以看作是以非线性方式对输入进行扭曲，使类别在最后一层变得可线性分离(图.1)。

​		在20世纪90年代，神经网络和反向传播很大程度上被机器学习界锁抛弃同时也被计算机视觉和语音识别界锁抛弃。人们普遍认为，没有先验知识的条件下，以学习为目的，多阶段的特征提取器是无用的。特别是，人们普遍认为，简单的梯度下降会使学习器陷入局部最小值——权重的任何配置都不会减少平均误差。

​		在实际中，一个差的局部最小值很少能成为大型网络的问题。无论初始条件如何，系统几乎总是能达到质量相似的解决方案。最近的理论和经验结果证明，局部最小值往往通常不是一个严重的问题。相反大量其他的组合比如说梯度为0的鞍点问题，鞍点是指在大多数维度上向上，在其他维度向下的图形。分析似乎表明，有很多鞍点只有很少的向下的去此案，但是几乎所有这些鞍点都有相似的目标函数值。因此并不需要关心这些函数是卡在哪个鞍点上。

​		2006年左右，在加拿大高级研究生(CIFAR)的召集下，深度前馈网络重新激发了他们的兴趣。研究人员引入了无监督学习程序，可以在不需要标记数据的情况下创建特征检测器层。学习每一层特征检测器的目标是能够重建或者建模下一层检测器(或者原始输入)的活动。通过预训练更加复杂的特征提取器来重建目标，深度网络的权重可以被初始化为合理的值。最后一层的输出单元可以被添加到网络的顶部，整个深度系统可以使用标准的反向传播进行微调。这在识别手写数字或检测行人方面效果特别好，特别是当标签数据量非常有限时。

​		这种预训练方法第一个主要应用是语音识别，他的成功和快速图像处理单元(GPUs)特别有关,GPUs特别容易编程，并且可以让研究者处理图像的速度快上10-20倍。在2009年，这种方法被用来从声波中提取的系数短时窗口映射到一组可能由窗口中心的帧所代表的各种语音片段的概率。它在使用小词汇量的标准语音识别基准上获得了突破性的结果，并且迅速在大词汇量上开发，也获得了小词汇给定的突破性的结果。到2012年，许多主要的语音团队基于2009年开发的深度网络版本被部署到Android手机上。对于小的数据集，非监督学习预训练克服了过拟合，当标签例子较少时，或者"源"任务的数据量比较多，但是"目标"任务的数据量比较少时取得很好的泛化效果，一旦深度学习得到恢复，就会发现，预训练阶段只要小数据集。

​		但是，有一种很特别的深度，前馈网络比全连接的相邻层更容易训练，更好去泛化。这就是卷积神经网络 (ConvNet)。在神经网络不收欢迎的时期，它就能取得实践上的成功，现在他被计算机视觉广泛接受。

### 卷积神经网络 (Convolutional neural networks)

卷积网络被设计用来处理多个数组形式出现的数据，例如，一幅彩色图像由三个二维数组组成，三个二维数组就是包含像素强度的三通道数据。许多数据方式都是以多个数组的形式出现的。1D的信号和序列，包括语言，2d的图片或者音频频谱，3D的视频或体积图像。ConvNets背后有四个关键思想，利用自然信号的特性，局部连接，共享权重，赤化，和使用很多层。

一个典型的ConvNet架构(图2)，是由一系类阶段组成的。前几个阶段是由两种类型层组成 : 卷积层和池化层。卷积层中的单元被组织在特征图中，每一个单元通过滤波库连接到在之前层特征图的局部补丁上。这个单元加权和的结果被传到例如ReLU的非线性函数上。特征图中的所有单元共享同一个滤波库。一个图上不同的特征使用不同的滤波库。为什么要用这种架构有两个方面:第一，在图像等数组数据中，局部的值往往互相关联，形成容易被检测的独特的局部图案。第二，图像和其他信号的局部统计量相对不变的。换句话说，如果一个团出现图像的某一部分，那么它可能出现任何地方，因此，不同位置的单元共享相同的权重，在阵列的不同部分检测相同的图案。从数学上将，特征图进行的滤波操作，称为离散卷积，遂以此命名。

虽然卷积层的作用是检测上一层特征的局部连接，池化层的作用是将语义相同的特征合并成一个特征。一个典型的池化单元计算一个特征图(或者是新的特征图)中某个单元的小块最大值。相近的池化单元从小块偏移超过一行或者一列中获取输入，从而降低了表示表征维度并且对小的偏移和扭曲产生不变性。

卷积，非线性函数和和池化的两三个阶段被堆叠起来。紧接着是更多的卷积和全连接层。通过ConvNet反向梯度就像常规的深度学习网络一样简单，可以训练滤波器中的所有权重。

深度神经网络利用了许多自然信号的特性都是组合层次的，其中高层次的特征是由较低层次的特征组合而成的。在图像中，图片边缘的局部图案，图案组成一块块的更多的图案，这些一块块的图案形成一个个对象。相似的层级结构也存在于声音到音素，音节，单词和句子中。当前一层中的元素在位置和外观上发生了变化，池中的表征不会产生很大的变化。

ConvNets中的卷积和池化层是受到视觉神经中，简单细胞和复杂细胞的启发，整体架构让人想起视觉皮层腹中的LGN-V1-V2-V4-IT层次结构。当ConvNet模型和猴子看到同一张图片时，ConvNet中高级单位的激活解释了猴子的下颞皮层160个神经元随机集的一半方差。ConvNets起源于新认知，其架构有点相似，但是没有端到端的例如反向传播的监督学习。一个被称为延时神经网的一维ConvNet可以用来识别音素和简单单词。

从早期的20世纪90年代起，卷积神经网络就有数千万的应用。从用于语音识别和文档阅读的时延神经网络。文档阅读系统使用一个ConvNet与实现语言约束的概率模型联合训练。到20世纪90年代末，这个系统已经读取了10%的美国支票。后来微软部署了基于ConvNet的光学字符识别和手写识别系统。在20世纪90年代早期，ConvNet还被用于自然图像的物体检测，包括手和脸以及被用于人脸识别。

![N7N25t.png](https://s1.ax1x.com/2020/07/01/N7N25t.png)

**图3|从图片到文字** 递归神经网络(RNN)生成字幕，作为额外的输入，从一个测试图片中提取深度卷积神经网络作为表征。RNN尝试将高纬表征训练成字幕(顶层)。经授权转载自参考文献.102.当RNN在生成每个单词(粗体)时，它需要把注意力集中在输入图像的不同位置(中间和底部，越亮的地方有更多的注意力)，我们发现可以利用这个获取图片到文字更好的翻译。

### 深度卷积网络的图像理解

自从2000年以来，ConvNet被成功的应用于图片中物体的检测，分割和识别。这些都是标签数据相对丰富的任务，比如交通灯识别，生物图片的分割，特别是脑神经的链接组学，面部识别，文字，行人，自然图片的人体。最近较为成功的例子就是脸部识别。

重要的是，图像可以再像素级进行标记，这将在技术上有所应用，包括自主移动的机器人，自动驾驶的汽车，类似于Mobileye，NVIDIA都会在他们将来的视觉系统中使用基于ConvNet的技术。其他越来越重要的应用包括自然语言理解和语音识别。

尽管取得了这些成功，直到2012年的imageNet竞赛中，ConvNets一直还是被主流的计算机视觉和机器学习界所遗忘。当深度卷积网络应用在有1000个类的大约百万张图片的时候，他们取得了惊人的成绩，结果，几乎将最佳竞争产品的错误率降低了一半。成功的原因主要是它有效的使用了GPU是，ReLUs，一种叫做dropout的正则化技术，已经通过变现现有的例子来生成新的训练样本。这种成功给计算机视觉代理革命，ConvNets现在几乎是所有识别和检测任务的主流方法，并在一些方面接近人类的表现。最近一个令人惊叹的演示是将ConvNets和循环网络模块用于图片字母的生成(图3)。

最近ConvNet架构有10-20层的ReLU架构，数亿个权重，单元之间有十亿个连接。两年前训练如此庞大的网络需要几周的时间，但在硬件、软件和算法并行优化可以将这么多时间缩短到几个小时。

基于ConvNet视觉的表现， 大多数技术公司，包括谷歌，Facebook，Microsoft,IBM,Yahoo,Twitter和Adobe,还有一大部分初创企业纷纷启动研发项目，并部署基于ConvNet的图片理解产品和服务。

ConvNet很容易适应有效的芯片硬件实现，或者可编程门阵列实现。很多公司比如NVIDIA，Mobileye，Intel，Qualcomm和三星都开发了在智能手机，相机，机器人和自动驾驶汽车上的可以安装实时系统的ConvNet芯片。

![NqnFUS.png](https://s1.ax1x.com/2020/07/02/NqnFUS.png)
![NqniE8.png](https://s1.ax1x.com/2020/07/02/NqniE8.png)

**图4|可视化学习单词向量** 左边的图片是模型语言学习到的词向量表征，可以用t-SNE去学习2维的可视化，右边的图是英语到法语的编码器-解码器的循环神经网络。可以观察到，语义相似的单词或单词序列可以被映射到附近的表征上。词的分布式表征是通过使用反向传播来共同学习每个词的表征和一个预测目标量的函数，如序列中的下一个词（用于语言建模）或整个翻译词序列（用于机器翻译）来获得的。

![NqKdhD.png](https://s1.ax1x.com/2020/07/02/NqKdhD.png)

**图5|循环神经网络及其前向计算所涉及的计算在时间上的展开** 人工神经元（例如，在节点s下分组的隐藏单元，在时间t处的值为st）在之前的时间步（用黑色正方形表示，表示一个时间步的延迟，在左侧）从其他神经元获得输入。这样，一个递归神经网络就可以把一个含有xt元素的输入序列映射成一个含有ot元素的输出序列，每个ot依赖于所有先前的xtʹ（对于tʹ≤t）。每个时间步使用相同的参数（矩阵U、V、W）。许多其他的架构是可能的，包括一个变体，其中网络可以生成一系列输出（例如，字），每个输出被用作下一个时间步的输入。反向传播算法（图1）可直接应用于右侧展开网络的计算图，以计算关于所有状态st和所有参数的总误差（例如，生成正确输出序列的对数概率）的导数。

### 分布式表示与语言处理

深度学习表明，与传统的不使用分布式表示的系统相比，深度学习有两个不同的指数优势。这两个优势都来自于组成的力量并取决于底层数据生成分布具有适当的成分结构。首先，学习分布式表征能够泛化到学习特征值的组合，超出训练期间看到的组合(例如，2n种组合是可能的n个二进制特征)，其次，在深王中组成层的表征带来了一种指数优势的潜力。

多层网络的的隐藏层以比较容易的预测目标输出的方式学习网络输入的表征。这一点可以被很好的证明，一个多层神经元网络预测句子中的下一个词，他会考察上下文中之前的词。上下文中的每一个词都被以1*N的向量传递到网络当中，每一个分量，要么是1要么是0。在第一层，每一个单词都会创造不同的激活模式，或者说词向量(图4)，在语言模型中，网络的其他层，将输入向量转化为下一个词的输出向量，这就可以被用来预测单词表中下一次出现的概率。网络学习词向量，词向量包含很多活跃的组件，其中每个组件都可以被解释单词独特的特征，就像学习符号的分布式表征的首次证明一样。这些语义特征并没明确放到输入中去。它们被学习程序发现，作为将输入和输出符号之间的结构关系因子化为多个 "微规则 "的好方法。结果证明，学习词向量在真实文本的大语料库的句子中工作良好，就算单个的微规则不可靠也行。当训练一个新的故事的下一个词，比如，要学习的词在周二周三非常的单词向量相似，挪威和瑞典也非常相似。这种表征被称为分布式表征，因为它们的元素（特征）并不相互排斥，它们的许多配置对应于观察到的数据中的变化。这些学习特种组成的词向量不是由专家提前确定的，是由神经网络自动发现的。单词的向量表示在神经语言应用中被广泛使用。

表征问题的引起争论的核心是逻辑启发还是神经网络启发作为识别的范例。在逻辑启发的范例中，一个符号实例是判断是否与其他实例相同还是不相同。它没有与其使用相关的内部结构；而要对符号进行推理，就必须将其与明智地选择的推理规则中的变量进行绑定。与之相对，神经网络就是用的大的活动矩阵，大的权重矩阵和可变的非线性去执行快速推理，而这种推理是毫不费力的常识推理的基础。

在介绍神经语言模型之前，语言统计建模的标准方法没有利用分布式来表示，他基于计算长达N的短的符号序列(N-grams)出现的频率。可能的N-grams数量在VN的数量级，其中V是单词的数量，因此考虑到超过少数单词的语境将需要非常大的训练语料库。N-gram将每一个单词作为一个原子单元，所以他们不能在语义相同的句子中进行泛化，但神经语言模型则可以，因为他们讲每个单词与实值特征相关联，语义相关的单词在该向量空间中最终接近。

### 循环神经网络

当反向传播刚刚开始被引入时，它最兴奋的应用就是循环神经网络(RNNs).对于涉及顺序输入的任务，如语音或者语言，最好使用RNNs(图5)。RNNs每次只处理一个元素的输入序列，在它们的隐藏单元中维护着一个"状态向量"，"状态向量"隐含的包含序列中所有的历史信息。当我们把隐藏单元在不同离散时间步长的输出看作是深层多层网络中不同神经元的输出时（图5，右），我们如何应用反向传播来训练RNN就变得很清楚了。

RNNs是非常强大的动态系统，但是训练它们被证明是有问题的，因为每一层的训练中，反向传播的梯度会增长或者收缩，以至于多次训练之后梯度会爆炸或者消失。

由于架构和训练它们的方法的进步，RNNs已经被发现在预测文本中的下一个字符或者句子中的下一个单词有良好的表现。但是它们也可以用于更复杂的任务。例如，在一次一个单词阅读句子之后，一个英文的'编码器'网络可以训练成最终隐藏单元的状态向量可以很好的表达句子的思想。然后，这个想法向量可以很好地被用来作为‘法语’解码器的初始状态(或者额外的输出)。该网络输出法语翻译的第一个单词的概率分布。如果从这个分布中选择一个特定的初始单词，并且作为输入到解码器网络，他会输出翻译中第二单词的可能性分布。如此往复，知道选择一个完整的停止。总之，这是一个取决于英语句子的概率分布来生成法语单词的序列。这是一个相对天真的机器翻译的方式已经迅速成为最先进的竞争力的方式，这就引起了严重的怀疑，理解一个句子是否需要使用想推理规则的内部符号表达。它更符合这样的观点，即日常推理涉及许多同时进行的类比，每个类比都为一个结论贡献了合理性。

与其把法语句子翻译成英语句子，不如学习将突破的意思"翻译"成英语句子(图3)。这里的编码器是一个深度ConvNet,他能把像素在最后的隐藏层中转化成活动向量。解码器是一个月机器翻译和神经语言模型相似的RNN结构，最近，人们对这种系统的兴趣大增。(参见文献86中提到的例子)

RNNs一旦在时间上展开(图5),可以看做是非常深的前馈网络，所有层都共享相同的权重。  虽然他们的主要目的是学习长期的依赖关系，但理论和经验证据表明，很难学习存储信息很长的时间。

为了纠正这一点，一个想法是用显示内存扩大网络。这种放的第一种是使用特殊的隐藏单元的长短期记忆网络(LSTM)一种被称为记忆门，他想累加器或者门控漏电的神经单元。它在下一时刻自我连接，并且只有一个权重，因此他复制自己的实值状态并累加外部信号，但这个自连接被英英单元乘法门控制，它会学习何时清楚记忆内容。

LSTM后来被证明比传统的RNN更有效，尤其是当他们在每个时间都有很多层时，能够实现从声学到转录中字符序列的整个语音识别系统。LSTM网络或相关形式的选通单元目前也用于编码器和解码器网络，它们在机器翻译方面表现得非常好。

在过去的一年里，有几位作者提出了不同的建议，用记忆模块来增强RNNs。这些建议包括神经图灵机(Neural Turing Machine)和记忆网络(memory networks)，前者通过"磁带式"的存储器来增强网络，RNN可以选择从该存储器来读取和写入，后者通过一种关联存储来增强常规网络。记忆网络在标准问答测试中取得了很好的效果。这些记忆网络中的内存是用来记住故事，来回答问题。

除了简单的记忆之外，神经图灵机和记忆网络正在被用于需要推理和符号操作的任务。神经图灵机可以被教会一些算法。其中，当他们的输入由一个未排序的符号列表，其中每个符号都伴随着一个表示其在表中优先级的实数值。记忆网络可以被训练成在阅读一个故事后，它们可以跟踪世界状态，回答需要复杂推理的问题。在一个测试的例子中，网络看到了15个句子的版本的<<指环王>>，并正确回答了"Frodo现在在哪"的问题。

### 深度学习的未来

无监督学习对重载人们对深度学习的兴趣有促进作用，后来被 纯监督学习的成功所掩盖，尽管我们不能在这篇review中看到他，但我们预计监督学习在长期会变得更加重要。人和动物的学习大都是无监督的：我们发现这个世界的方式是通过观察，而不是被告知，每个东西叫什么名字。

人类的视觉是一个主动的过程，以一种智能的，特定任务的方式对未来视觉镜像采样，它使用一个小的，高分辨率的的凸起和一个大的，低分辨率的环绕。我们预计未来视觉领域的大部分进展将来自于端到端训练的系统，并将ConvNets与RNNs结合起来，使用强化学习来决定看哪里。结合深度学习和强化学习的系统还处于起步阶段，但它们已经在分类任务中胜过被动视觉系统， 并在学习玩许多不同的视频游戏方面产生了令人印象深刻的结果。

自然语言处理是另一个深度学习在未来几年产生影响的领域。我们预计，当使用RNNs来理解句子或者整片文章，他会有选择地关注一个部分的策略，它也会更出色。

最终，人工智能的主要进展将通结合表征学习和复杂推理的系统来实现。尽管相当一段时间内深度学习和简单推理都用于语音和手写识别，但需要新的范式来取代基于规则的符号表达式操作，对大向量进行操作。
